# FSDP-specific configuration for OLMo training

# FSDP configuration
mixed_precision: true              # Whether to use mixed precision
shard_strategy: "FULL_SHARD"       # Sharding strategy (FULL_SHARD, SHARD_GRAD_OP, or NO_SHARD)
cpu_offload: false                 # Whether to offload parameters to CPU
backward_prefetch: true            # Whether to prefetch gradients for backward pass
min_shard_size: 1024000            # Minimum shard size in bytes

# Optimizer settings for FSDP
use_fp32_optimizer: false          # Whether to use FP32 optimizer state
gradient_accumulation: 1           # Gradient accumulation steps
gradient_clipping: 1.0             # Gradient clipping value

# Checkpointing
sharded_checkpointing: true        # Whether to use sharded checkpointing
checkpoint_folder: "checkpoints"   # Folder for checkpoints
save_optimizer: false              # Whether to save optimizer state

# Performance tuning
activation_checkpointing: true     # Use activation checkpointing to save memory
activation_cpu_offload: false      # Offload activations to CPU

# Keep the global batch size the same
batch_size: 8  # This is likely batch_size * sequence_length = 8 * 1024 = 8,192

# Adjust micro-batch size to be 1/4 of the global batch
train_module:
  rank_microbatch_size: 2048  # Set to 1/4 of your global batch size (8,192 รท 4)
  max_sequence_length: 1024

# # Increase global batch size by 4x
# batch_size: 32  # 32 * 1024 = 32,768 tokens

# # Keep the original micro-batch size
# train_module:
#   rank_microbatch_size: 8192
#   max_sequence_length: 1024