# config.yaml

wandb_project: "all_in_olmo"

#------------------------------------------------------------------------------------------------
# General Important Parameters (Top-level)
#------------------------------------------------------------------------------------------------
sequence_length: 1024
# batch_size is a fallback if 'global_batch_size' in the Training Hyperparameters section is not specified.
# It represents the number of sequences in a global batch (for one optimizer step).
batch_size: 80
micro_batch_size: 80 # Number of sequences processed per device in a single forward/backward pass.
steps: 15

#set to - (minus) to disable evaluation and inference
evaluation_times: 10 # run evaluation 2 times (eval_interval = steps/evaluation_interval) + one time at the beginning
inference_times: 10   # Run inference 2 times (inference_interval = steps/inference_times)

# Group Query Attention (GQA)
n_kv_heads: 4                             # Number of key/value heads for GQA. Must be <= n_heads.
                                          # Make sure build_model passes this to TransformerConfig.
# Layer-Wise Scaling parameters

learning_rate: 0.001
# INCREASE LEARNING RATE for small runs <5B tokens

fnn_scalars: [1.0, 1.0]
qkv_scalars: [1.0, 1.0]
output_scalars: [1.0, 1.0]
#fnn_scalars: [1.0, 2.0]
#qkv_scalars: [0.5, 1.0]
#output_scalars: [0.8, 1.2]
verbose_scaling: false

wandb_name: "test"
train_data_file: "test.npy"


# -----------------------------------------------------------------------------
# directories config
# -----------------------------------------------------------------------------
data_dir: ./data_local_test                # Directory for prepared data and checkpoints
save_dir: checkpoints



#------------------------------------------------------------------------------------------------
# DATA PREPARATION ARGUMENTS
#------------------------------------------------------------------------------------------------
data_preparation:
  output_file_name: "test.npy" # Name of the tokenized file within data.data_dir
  validation_output_file_name: "c4_validation.npy"
  validation: False
  #tokenizer_name: "allenai/gpt-neox-olmo-dolma-v1_5" # assuming we're not changin tokenizer
  #hf_dataset_name: "wikipedia"
  #hf_dataset_subset: "20220301.en"
  total_tokens_to_collect: 1_500_000
  tokenizer_processing_batch_size: 50  # how many articles/entries to tokenize at once
  dataset_proportions:  
    ### SCIENCE small DATASETS - always included.
    allenai/ai2_arc/ARC-Challenge: 0.02
    allenai/ai2_arc/ARC-Easy: 0.02
    allenai/sciq: 0.02

    allenai/dolmino-mix-1124/wiki:   0.8
    #allenai/dolmino-mix-1124/dclm
    allenai/dolmino-mix-1124/flan: 0.2
    #allenai/dolmino-mix-1124/pes2o: 0.8
    
    ### CODE AND MATH DATASETS
    #tokyotech-llm/swallow-code/exp1-the-stack-v2: 1.0
    #math-ai/AutoMathText/code-full: 0.25
    #math-ai/AutoMathText/web-0.50-to-1.00: 0.5
    #math-ai/AutoMathText/arxiv-full: 0.5


#ds = load_dataset("allenai/ai2_arc", "ARC-Challenge")
#ds = load_dataset("allenai/ai2_arc", "ARC-Easy")
#7k rows * 100 tokens = 700k tokens

#ds = load_dataset("allenai/sciq")
#11k rows * 100 tokens = 1.1M tokens


### Coding dataset 
#ds = load_dataset("tokyotech-llm/swallow-code", "exp1-the-stack-v2")

### Math dataset
#ds = load_dataset("math-ai/AutoMathText", "web-0.50-to-1.00") # or any valid config_name
#ds = load_dataset("math-ai/AutoMathText", "code-full") # or any valid config_name
#ds = load_dataset("math-ai/AutoMathText", "arxiv-full") # or any valid config_name








#------------------------------------------------------------------------------------------------
# HF CACHE PATHS - default now, change if needed
#------------------------------------------------------------------------------------------------
hf_cache_paths:
  transformers: cache/transformers
  datasets: cache/datasets
  huggingface: cache/huggingface


#------------------------------------------------------------------------------------------------
# WANDB API KEY - put your own key.
#------------------------------------------------------------------------------------------------

WANDB_API_KEY: "8e07447fa3d6269331f7ecd0b27f8518c2a65855"


# -----------------------------------------------------------------------------
# Training Hyperparameters
# -----------------------------------------------------------------------------
# The following block was previously enclosed in """ which is not valid YAML for comments.
# Each line should be prefixed with # if it's a comment.
# If these are intended to be active parameters, they should be uncommented and correctly formatted.

# steps: 20                                 # Total number of global training steps (optimizer steps)
# global_batch_size: 2048                      # Total batch size across all GPUs.
#                                           # For FSDP, this will be divided among ranks.
#                                           # Example: 2 GPUs, global_batch_size 8 -> 4 sequences per GPU per global step.
#                                           # Your previous value was 2, which is very small for a global batch.
#                                           # Adjust based on your GPU memory and number of GPUs.
# sequence_length: 1024                     # Sequence length for the model
# 
# gradient_accumulation_steps: 1            # Number of micro-steps to accumulate gradients before an optimizer step.
#                                           # Effective batch size = global_batch_size * gradient_accumulation_steps (conceptually, though OLMo applies global_batch_size per optimizer step)
#                                           # OLMo's TrainerConfig uses global_batch_size directly for the optimizer step.
#                                           # The device_train_microbatch_size is what's processed per forward/backward pass on a device.
#                                           # Set device_train_microbatch_size in TrainerConfig or let OLMo infer.
#                                           # If OLMo infers, it's often global_batch_size / world_size / gradient_accumulation_steps.

# Assuming the parameters below this line are the ones you actually want active for this section:

#learning_rate: 0.001
# INCREASE LEARNING RATE for small runs <5B tokens
weight_decay: 0.1
betas: [0.9, 0.95]                        # AdamW betas
seed: 42                                  # Base seed for reproducibility (will be adjusted per rank in train.py)



# Note: The original config had 'steps', 'global_batch_size', 'sequence_length', 
# and 'gradient_accumulation_steps' defined both at the top-level (lines 6-9) 
# and within the commented-out block above. 
# Ensure the active definitions (e.g., lines 6-9 and 66-77) reflect your intended settings.
# If the block above (lines 57-71) was meant to be active, you need to remove the # from each line.
# For now, I am assuming the top-level definitions and the learning_rate, weight_decay etc. below are the active ones.


# -----------------------------------------------------------------------------
# Model Configuration (OLMo-190M base with GQA and LWS)
# -----------------------------------------------------------------------------
# These parameters define the model architecture.
# Ensure your `utils/model.py` correctly uses these values, especially n_kv_heads,
# and custom LWS scalars if they modify the TransformerConfig or model layers.
model_dtype: "bf16"                       # Precision for model parameters and computation (e.g., "bf16", "fp32").
                                          # This will guide dtype in build_model and FSDP settings.

# OLMo-190M defaults (can be overridden if build_model passes them to TransformerConfig)
global_dim: 768                           # d_model: Dimensionality of the model
n_heads: 12                               # Number of attention heads
head_dim: 64                              # Dimensionality of each attention head (global_dim / n_heads)






# -----------------------------------------------------------------------------
# FSDP and Activation Checkpointing
# -----------------------------------------------------------------------------
# FSDP settings are largely handled by TransformerDataParallelConfig in utils/model.py
# Activation checkpointing mode can be configured if needed (e.g., "full", "fine_grained")
# Defaulting to "full" as set in utils/model.py's TransformerTrainModuleConfig.
# activation_checkpointing_mode: "full" # Example if you want to control it from here.

# -----------------------------------------------------------------------------
# Inference Callback
# -----------------------------------------------------------------------------
#inference_interval: 20                     # Run inference every N global steps
inference_prompt: "The universe is "


# global_dim: 768
# head_dim: 64

